{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas\n",
    "\n",
    "path = '../crawler/ksou/data/description/'\n",
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "count = 0\n",
    "LIMIT = 1000\n",
    "re_number = re.compile(r'\\d')\n",
    "\n",
    "def has_number(string):\n",
    "    return re_number.search(string) != None\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    stems = [s for s in stems if not has_number(s)]\n",
    "#    print(stems)\n",
    "    return stems\n",
    "\n",
    "def getResult():\n",
    "    global count\n",
    "    for file in os.listdir(path):\n",
    "        with open(path+file, 'rb') as f:\n",
    "            text = f.read().decode('utf-8')\n",
    "    #        print(text)\n",
    "            if len(text) > 20:\n",
    "                token_dict[file] = text.translate(translator)\n",
    "                count += 1\n",
    "                if count >= LIMIT:\n",
    "                    break\n",
    "    \n",
    "    #this can take some time\n",
    "    print('Finish collecting the doc... training using tfidf')\n",
    "    tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english', lowercase=True)\n",
    "    tfs = tfidf.fit_transform(token_dict.values())\n",
    "    dense = tfs.todense()\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "    doc_names = token_dict.keys()\n",
    "    return dense, feature_names, doc_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish collecting the doc... training using tfidf\n"
     ]
    }
   ],
   "source": [
    "dense, feature_names, doc_names = getResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thi          828\n",
       "bedroom      817\n",
       "home         697\n",
       "kitchen      645\n",
       "bathroom     615\n",
       "area         585\n",
       "live         520\n",
       "locat        512\n",
       "featur       506\n",
       "block        458\n",
       "room         447\n",
       "open         438\n",
       "face         389\n",
       "floor        381\n",
       "larg         380\n",
       "secur        360\n",
       "land         359\n",
       "frontag      358\n",
       "metr         356\n",
       "park         356\n",
       "backyard     354\n",
       "loung        345\n",
       "level        345\n",
       "view         343\n",
       "ha           342\n",
       "plan         341\n",
       "properti     339\n",
       "famili       336\n",
       "air          336\n",
       "slope        334\n",
       "            ... \n",
       "south        133\n",
       "bath         129\n",
       "ideal        128\n",
       "contact      127\n",
       "stainless    127\n",
       "style        126\n",
       "stun         123\n",
       "steel        123\n",
       "meal         122\n",
       "ground       121\n",
       "aircondit    119\n",
       "window       118\n",
       "today        117\n",
       "fantast      115\n",
       "distanc      115\n",
       "entri        115\n",
       "dishwash     114\n",
       "carport      113\n",
       "complet      112\n",
       "buyer        111\n",
       "year         110\n",
       "price        110\n",
       "pleas        110\n",
       "short        108\n",
       "bay          107\n",
       "inform       107\n",
       "care         105\n",
       "shed         102\n",
       "cupboard     101\n",
       "owner        101\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.DataFrame(dense, index = doc_names, columns = feature_names)\n",
    "df_non_zero = df[df !=0]\n",
    "df_non_zero_sum = (df !=0).sum(axis = 0)\n",
    "# df.to_csv('./result/'+'100_tfid_result.csv')\n",
    "df_non_zero_100 = df_non_zero_sum[df_non_zero_sum > 100].sort_values(ascending=False)\n",
    "df_non_zero_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './result/100_words.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-55ccb332c4c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnon_zero_100_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_non_zero\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_non_zero_100\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnon_zero_100_cols_average\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_zero_100_cols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdf_non_zero_sum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_non_zero_100\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnon_zero_100_cols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./result/100_words.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\formats\\format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1458\u001b[0m             f = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1459\u001b[0m                             \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m                             compression=self.compression)\n\u001b[0m\u001b[1;32m   1461\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path, mode, encoding, compression, memory_map)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './result/100_words.csv'"
     ]
    }
   ],
   "source": [
    "non_zero_100_cols = df_non_zero[df_non_zero_100.index]\n",
    "non_zero_100_cols_average = non_zero_100_cols.sum(axis = 0)/df_non_zero_sum[df_non_zero_100.index]\n",
    "non_zero_100_cols.to_csv('./result/100_words.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
